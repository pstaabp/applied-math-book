<?xml version="1.0" encoding="UTF-8"?>

<section xml:id="sect-linear-transformations"><title>Linear Transformations</title>

  <p>This section discusses linear transformations.  In short, such a
    transformation will map vectors to vectors in a linear way.  The definition
    of a linear transformation (or linear map) is the following: </p>

  <definition xml:id="def-linear-trans">
    <title>Linear Transformation</title>
    <statement>
      <p>
   Let <m>V</m> and <m>W</m> be vector spaces.  A \textbf{linear transformation} or \textbf{linear map} <m>T</m> from <m>V</m> to <m>W</m> is a function that assigns to each vector <m>\vec{v} \in V</m> a unique vector <m>T\vec{v} \in W</m> and that satisfies for each <m>\vec{u}</m> and <m>\vec{v}</m> in <m>V</m> and each scalar <m>\alpha</m>,

  <md>
   <mrow xml:id="eq-linear-map-add" number="yes">(\vec{u} + \vec{v}) \amp = T(\vec{u}) + T (\vec{v}), </mrow>
    <mrow xml:id="eq-linear-map-prod" number="yes">T(\alpha \vec{v}) \amp = \alpha T (\vec{v}), </mrow>
  </md>
      </p>
    </statement>
  </definition>

  <p>These are also called <term>homomorphisms</term> and the notation
    explaining that a map <m>T</m> goes from <m>V</m> to <m>W</m>
    is <m>T: V \rightarrow W</m>. </p>

  <example xml:id="ex-reflect-map">
    <title>Reflection Map</title>
    <statement>
      <p>
  The reflection of any vector in <m>\mathbb{R}^2</m> across the horizontal axis is a linear map.  Specifically this is given as

  <md>
  T(\begin{bmatrix}
  x_1 \\ x_2
  \end{bmatrix}) \amp = \begin{bmatrix}
  x_1 \\ -x_2
  \end{bmatrix}
  </md>
  and geometrically you can see this as: </p>

  <figure xml:id="fig-reflection-map">
    <caption>Geometric Reflection Map</caption>
    <image width="50%" xml:id="plot-reflection-map">

  <latex-image>

  \begin{tikzpicture}[scale=1.5]

  \draw[->] (-1,0) -- (2,0);  \draw[->] (0,-1) -- (0,1);
  \draw[->, thick] (0,0) -- (1.5,0.5) node [above right] {$\displaystyle \begin{bmatrix}
  x_1 \\ x_2
  \end{bmatrix}$};

  \draw[->, thick] (0,0) -- (1.5,-0.5) node [below right] {$\displaystyle \begin{bmatrix}
  x_1 \\ -x_2
  \end{bmatrix}$};
  \end{tikzpicture}
</latex-image>
</image>
</figure>

  <p>Show that this is a linear transformation.      </p></statement>
  <solution>

  <p> Specifically, we need to show that <m>T</m> defined above
    satisfies (<xref ref="eq-linear-map-add" />) and (\ref{eq:linear:map:prod}).

  Let

  <md>
  \vec{x} \amp = \begin{bmatrix}
  x_1 \\ x_2
  \end{bmatrix}, \amp \vec{y} \amp = \begin{bmatrix}
  y_1 \\ y_2
  \end{bmatrix}
  </md>
  and <m>\alpha \in \mathbb{R}</m>, then

  <md>
  T(\vec{x}+\vec{y}) \amp = T \biggl( \begin{bmatrix}
  x_1 \\ x_2
  \end{bmatrix}+ \begin{bmatrix}
  y_1 \\ y_2
  \end{bmatrix} \biggr) = T \biggl( \begin{bmatrix}
  x_1 + y_1 \\ x_2 + y_2
  \end{bmatrix} \biggr) \\
  \amp = \begin{bmatrix}
  x_1 + y_1 \\ -(x_2+y_2)
  \end{bmatrix} = \begin{bmatrix}
  x_1 + y_1 \\ -x_2 -y_2
  \end{bmatrix} \\
  \amp = \begin{bmatrix}
  x_1 \\ -x_2
  \end{bmatrix} + \begin{bmatrix}
  y_1 \\ -y_2
  \end{bmatrix} = T(\vec{x}) + T(\vec{y}).
  </md>
  so (<xref ref="eq-linear-map-add" />) is satisfied.  Next,

  <md>
  T(\alpha \vec{x}) \amp = T \biggl( \alpha \begin{bmatrix}
  x_1 \\ x_2
  \end{bmatrix} \biggr) = T \biggl( \begin{bmatrix}
  \alpha x_1 \\ \alpha x_2
  \end{bmatrix} \biggr) \\
  \amp = \begin{bmatrix}
  \alpha x_1 \\ -\alpha x_2
  \end{bmatrix} = \alpha \begin{bmatrix}
  x_1 \\ -x_2
  \end{bmatrix} = \alpha T(\vec{x}).
  </md>
  so <xref ref="eq-linear-map-prod" />) is satisfied.
  </example>

  A very important linear map in <m>\mathbb{R}^2</m> is the rotational map that takes any vector in the plane and rotates it a given angle.  The next example, derives this map.

  <example> \label{ex:rotational:trans}
   Let $\displaystyle \vec{x} =
  \begin{bmatrix}
   x_1 \\ x_2
  \end{bmatrix}<m> be a vector in </m>\mathbb{R}^2<m>.  Let the function </m>T<m> take the vector </m>\vec{x}<m> and rotate it by </m>\theta<m> radians in the counterclockwise direction.  Call the new vector </m>\displaystyle \vec{y} =
  \begin{bmatrix}
   y_1 \\ y_2
  \end{bmatrix}$.

  \begin{center}
  \begin{tikzpicture}[scale=1.5]

  \draw (0.2,0.2) node [above right] {<m>\theta</m>};
  \draw[->] (-2,0) -- (2,0);  \draw[->] (0,-2) -- (0,2);
  \draw[->, thick] (0,0) -- (1.5,0.5);   \fill (1.5,0.5) circle (1pt) node [above right] {<m>(x_1,x_2)</m>};

  \draw[->,thick] (0,0) -- (0.5,1.5); \fill (0.5,1.5) circle (1pt) node [above right] {<m>(y_1,y_2)</m>};

  \draw (0.3,0.1) arc [start angle={atan(1/3)}, end angle={atan(3)}, radius={sqrt(0.1)}];

  \draw (0.45,0) arc [start angle=0, end angle={atan(1/3)}, radius=0.45];

  \draw (0.6,0) node [above right] {<m>\alpha</m>};
  \end{tikzpicture}
  \end{center}

  Derive the formula for the rotational map.

  \solution

  Let <m>r = ||\vec{x}||</m> and since <m>\vec{y}</m> is the rotated version of <m>\vec{x}</m> it has the same length, therefore <m>r=||\vec{y}||</m>.  The values <m>x_1, x_2, y_1,y_2</m> can be written in terms of <m>\alpha</m>, the angle that the vector <m>\vec{x}</m> makes with the positive horizontal axis, and <m>\theta</m> the angle between the vectors as follows.
  <md>
   x_1 \amp = r \cos \alpha, \amp x_2 \amp = r \sin \alpha, \\
   y_1 \amp = r \cos (\alpha+\theta), \amp y_2 \amp= r \sin (\alpha + \theta) \\
   \amp = r \cos \alpha \cos \theta - r \sin \theta \sin \alpha, \amp y_2 \amp = r \sin \theta \cos \alpha + r \sin \alpha \cos \theta \\
   \amp = x_1 \cos \theta - x_2 \sin \theta, \amp \amp = x_1 \sin \theta + x_2 \cos \theta
  </md>
  and note that these can be written:

  <md>
  \begin{bmatrix}
   y_1 \\ y_2
  \end{bmatrix} \amp =
  \begin{bmatrix}
   \cos \theta \amp - \sin \theta \\ \sin \theta \amp \cos \theta
  \end{bmatrix}
  \begin{bmatrix}
   x_1 \\ x_2
  \end{bmatrix}
  </md>
  This is a linear transformation (as we will explain later) and is called a \textbf{rotational transformation}.
  </example>

  <example> \label{ex:scale:map}
  Consider the map <m>S: \mathbb{R}^2 \rightarrow \mathbb{R}^2</m>  that scales any vector in the plane by a factor of <m>k</m> given by
  <md>
  S(\vec{x}) = k \vec{x}
  </md>
  which can be visualized in the following diagram where <m>k=2</m>:

  \begin{center}
  \begin{tikzpicture}[scale=0.8]
  \draw[->] (-4,0) -- (4,0) node [above right] {<m>x</m>};
  \draw[->] (0,-4) -- (0,4) node [above right] {<m>y</m>};
  \foreach \x in {-3,-2,-1,1,2,3} {\draw (\x,0.15) -- (\x,-0.15) node [below] {<m>\x</m>};}
  \foreach \y in {-3,-2,-1,1,2,3} {\draw (0.15,\y) -- (-0.15,\y) node [left] {<m>\y</m>};}

  \draw[->] (0,0) -- (1,1); \draw[->,thick] (0,0)  -- (2,2);
  \draw[->] (0,0) -- (2,1); \draw[->,thick] (0,0)  -- (4,2);

  \draw[->] (0,0) -- (-1,1.5); \draw[->,thick] (0,0)  -- (-2,3);

  \draw[->] (0,0) -- (-1.5,-1); \draw[->,thick] (0,0)  -- (-3,-2);

  \end{tikzpicture}
  \end{center}
  where every vector under the map results in a new vector that is twice as long as the original.  In general, the scale <m>k</m> will scale the vector by a factor of <m>k</m> and recall that if <m>k<0</m>, then the direction changes.    Show that this is a linear map.

  \solution

  Again, we show that (<xref ref="eq-linear-map-add" />) and (\ref{eq:linear:map:prod}) are satisfied.   Let <m>\vec{x}</m> and <m>\vec{y}</m> be elements of <m>\mathbb{R}^2</m>.

  <md>
  S(\vec{x}+\vec{y}) = k(\vec{x}+\vec{y}) = k\vec{x} + k \vec{y} = S(\vec{x}) + S(\vec{y})
  </md> so (<xref ref="eq-linear-map-add" />) is satisfied and
  <md>
  S(\alpha \vec{x}) = k (\alpha \vec{x} ) = \alpha (k \vec{x}) = \alpha S(\vec{x})
  </md>
  so (<xref ref="eq-linear-map-add" />) is satisfied so <m>S</m> is a linear map.

  </example>




  <theorem> \label{thm:matrix:linear:trans}
  If a transformation map <m>T</m> is written as a matrix or <m>T(\vec{x}) = A\vec{x}</m>, then <m>T</m> is a linear transformation.
  </theorem>

  <proof>
  This is a consequence of matrix operations.

  <md>
  T(\vec{x}+\vec{y}) = A(\vec{x}+\vec{y}) = A\vec{x} + A \vec{y} = T(\vec{x}) + T(\vec{y}) \\
  T(\alpha \vec{x}) = A (\alpha \vec{x}) = \alpha A\vec{x} = \alpha T(\vec{x}).
  </md>
  </proof>



  Note that the rotational transformation that was defined in Example <xref ref="ex-rotational-trans" /> is easily shown to be a linear transformation because from Theorem \ref{thm:matrix:linear:trans}, any transformation shown as a matrix, is a linear transformation.

  The next theorem shows the counter direction to Theorem <xref ref="thm-matrix-linear-trans" />, that is that any linear transformation can be written as a matrix.


  <theorem> \label{thm:linear:trans:matrix}
   Let <m>T: \mathbb{R}^n \rightarrow \mathbb{R}^m</m> be a linear transformation.  Then there exists a unique <m>m</m> by <m>n</m> matrix <m>A_T</m> such that

  <md>
   T \vec{x} \amp = A_T \vec{x} \qquad \qquad \text{for every <m>\vec{x} \in \mathbb{R}^n.</m>}
  </md>
  </theorem>

  We won't prove this here, but instead will motivate this below.  In short, if <m>T</m> is a linear transformation, then the matrix <m>A_T</m> corresponding to the linear transformation is called the \textbf{transformation matrix}.



  <subsection><title>Finding the Matrix Form of a Linear Transformation</title>

  The theorem above shows that any linear transformation, <m>T: V \rightarrow W</m> can be written in matrix form.  This section explains how to find it.    Let <m>B_V=(\vec{v}_1,\vec{v}_2, \ldots, \vec{v}_m)</m> be a basis of <m>V</m> and <m>B_W=(\vec{w}_1,\vec{w}_2, \ldots, \vec{w}_n)</m> be a basis of <m>W</m>.  Any vector <m>\vec{x}</m> in <m>V</m> can be written

  <md>
  \vec{x} \amp = c_1 \vec{v}_1 + c_2 \vec{v}_2 + \cdots + c_m \vec{v}_m = \sum_{j=1}^m c_j \vec{v}_j
  </md>
  or in other words <m>\text{Rep}_{B_V}(\vec{x}) = \vec{c}</m>.   Applying the map to <m>\vec{x}</m> is

  <md>
  T(\vec{x}) \amp = T\biggl(\sum_{j=1}^m c_j \vec{v}_j \biggr) \notag
   \intertext{and since it is a linear map}
  \amp = \sum_{j=1}^m c_j T(\vec{v}_j)  \label{eq:lin:trans1}
  </md>

  Next, we write the transformation in terms of the basis vectors of <m>W</m> or

  <md> \label{eq:lin:trans2}
  T(\vec{v}_j) \amp = a_{1,j} \vec{w}_1 + a_{2,j} \vec{w}_2 + \cdots + a_{n,j} \vec{w}_n  = \sum_{i=1}^n a_{i,j} \vec{w}_i
  </md>

  Substituting (<xref ref="eq-lin-trans2" />) into (\ref{eq:lin:trans1}) results in

  <md>
  T(\vec{x}) \amp = \sum_{j=1}^m c_j \sum_{i=1}^n a_{i,j} \vec{w}_i
  = \sum_{i=1}^n \sum_{j=1}^m  a_{i,j} c_j \vec{w}_i
  </md>
  and letting <m>A_T</m> be the <m>m \times n</m> matrix with entries <m>a_{i,j}</m> then

  <md>
  \text{Rep}_{B_W} (T(\vec{x}))  \amp = A_T\vec{c}
  </md>
  or in other words, the matrix performs the map on the coefficients.

  Equation (<xref ref="eq-lin-trans2" />) also shows how the matrix <m>A_T</m> can be created from the linear map.  That equation can also be thought of as a representation of the basis vectors or

  <md>
  \text{Rep}_{B_W} ( T(\vec{v}_j)) \amp = \vec{a}_j
  </md>
  where <m>a_j</m> is the <m>j</m>th column of <m>A</m>.  The following summarizes how to find the matrix.

  <remark>
  If <m>A_T</m> is the matrix representation of the map <m>T</m>, then the <m>j</m>th column of <m>A_T</m>  is the vector <m>\text{Rep}_{B_W}(T(\vec{v}_j))</m>, the map applied to the <m>j</m>th basis vector of <m>V</m> written in terms of the basis of <m>W</m>.
  </remark>

  We now show many examples on how to apply this.

  <example>
  Find the matrix representation of the reflection map from Example <xref ref="ex-reflect-map" /> given by

  <md>
  T\biggl(\begin{bmatrix}
  x_1 \\ x_2
  \end{bmatrix} \biggr) \amp = \begin{bmatrix}
  x_1 \\ -x_2
  \end{bmatrix}
  </md>
  where the representation will be in terms of the standard basis vectors.

  \solution

  In this case, we need to determine how the standard basis vectors map under the reflection.  Thus

  <md>
  \vec{v}_1 \amp = \begin{bmatrix}
  1 \\ 0
  \end{bmatrix} \amp \vec{v}_2 \amp = \begin{bmatrix}
  0 \\ 1
  \end{bmatrix}
  </md>
  and

  <md>
  T(\vec{v}_1) \amp = \begin{bmatrix}
  1 \\ 0
  \end{bmatrix} \amp T(\vec{v}_2) \amp = \begin{bmatrix}
  0 \\ -1
  \end{bmatrix}
  </md>
  Because we are using the standard basis vectors, the representations of these vectors are themselves therefore,

  <md>
  A_T \amp = \begin{bmatrix}
  1 \amp 0 \\
  0 \amp -1
  \end{bmatrix}
  </md>
  and just to verify,

  <md>
  T\biggl(\begin{bmatrix}
  x_1 \\ x_2
  \end{bmatrix} \biggr) \amp = \begin{bmatrix}
  1 \amp 0 \\
  0 \amp -1
  \end{bmatrix} \begin{bmatrix}
  x_1 \\ x_2
  \end{bmatrix} = \begin{bmatrix}
  x_1 \\ - x_2
  \end{bmatrix}
  </md>

  </example>


  \phantom{hi}

  <example> \label{ex:scale:map:matrix:rep}
  Find the matrix representation of the scale map in Example <xref ref="ex-scale-map" />.

  \solution

  We need to map the basis vectors <m>[1\;\;0]^{\intercal}</m> and <m>[0\;\;1]^{\intercal}</m> to determine the columns of the matrix representation.

  <md>
  S \biggl(\begin{bmatrix}
  1 \\ 0
  \end{bmatrix} \biggr) \amp = \begin{bmatrix}
  k \\ 0
  \end{bmatrix} \amp S\biggl( \begin{bmatrix}
  0 \\ 1
  \end{bmatrix} \biggr) \amp = \begin{bmatrix}
  0 \\ k
  \end{bmatrix}
  </md>
  so the matrix representation is

  <md>
  A_S \amp = \begin{bmatrix}
  k \amp 0 \\
  0 \amp k
  \end{bmatrix}
  </md>
  </example>

  \phantom{hi}

  <example>
  Let <m>T: \mathcal{M}_{2 \times 2} \rightarrow \mathbb{R}</m>

  <md>
  \begin{bmatrix}
  a \amp b \\ c\amp d
  \end{bmatrix} \rightarrow a + d
  </md>
  which is the trace of a 2 by 2 matrix.  Show that the trace is a linear map and find the matrix representation of the trace.

  \solution

  First, to prove that the trace is a linear map, we need to show that it satisfies (<xref ref="eq-linear-map-add" />) and (\ref{eq:linear:map:prod}).  Let

  <md>
  A_1 \amp = \begin{bmatrix}
  a_1 \amp b_1 \\ c_1\amp d_1
  \end{bmatrix} \amp A_2 \amp = \begin{bmatrix}
  a_2 \amp b_2 \\ c_2\amp d_2
  \end{bmatrix}
  </md>
  Then

  <md>
  T(A_1+A_2) \amp = T \biggl( \begin{bmatrix}
  a_1 \amp b_1 \\ c_1\amp d_1
  \end{bmatrix} + \begin{bmatrix}
  a_2 \amp b_2 \\ c_2\amp d_2
  \end{bmatrix} \biggr) = T \biggl( \begin{bmatrix}
  a_1 + a_2 \amp b_1 + b_2 \\ c_1 + c_2 \amp d_1 + d_2
  \end{bmatrix} \biggr) \\
  \amp = (a_1 + a_2) + (d_1 + d_2) \\
  \amp = (a_1 + d_1) + (a_2 + d_2) = T(A_1) + T(A_2)
  </md>
  Similarly,
  <md>
  T(\alpha A_1) \amp = T \biggl( \alpha \begin{bmatrix}
  a_1 \amp b_1 \\ c_1\amp d_1
  \end{bmatrix}\biggr) = T\biggl(\begin{bmatrix}
  \alpha a_1 \amp \alpha  b_1 \\ \alpha c_1\amp \alpha d_1
  \end{bmatrix} \biggr) \\
  \amp = \alpha a_1 + \alpha d_1 = \alpha (a_1 + d_1) = \alpha T(A_1)
  </md>

  Next, we want to find the matrix representation of the trace.  To do this, we need to determine how the map affects the basis of the vector space <m>\mathcal{M}_{2 \times 2}</m>, which is

  <md>
  \bigl( \begin{bmatrix}
  1 \amp 0 \\ 0 \amp 0
  \end{bmatrix}, \begin{bmatrix}
  0 \amp 1 \\ 0 \amp 0
  \end{bmatrix}, \begin{bmatrix}
  0 \amp 0 \\ 1 \amp 0
  \end{bmatrix}, \begin{bmatrix}
  0 \amp 0 \\ 0 \amp 1
  \end{bmatrix} \bigr)
  </md>

  and since

  <md>
  \begin{bmatrix}
  1 \amp 0 \\ 0 \amp 0
  \end{bmatrix} \amp\mapsto 1, \amp
  \begin{bmatrix}
  0 \amp 1 \\ 0 \amp 0
  \end{bmatrix} \amp\mapsto 0, \\
  \begin{bmatrix}
  0 \amp 0 \\ 1 \amp 0
  \end{bmatrix} \amp\mapsto 0, \amp
  \begin{bmatrix}
  0 \amp 0 \\ 0 \amp 1
  \end{bmatrix} \amp\mapsto 1, \\
  </md>

  The matrix representation is

  <md>
  A_T \amp = \begin{bmatrix}
  1 \amp 0 \amp 0 \amp 1
  \end{bmatrix}
  </md>
  </example>

  To verify the above results, recall that from Example <xref ref="ex-vect-rep-matrix" /> that the representation of a matrix is the unfolded matrix or in the <m>2 \times 2</m> case, that

  <md>
  \text{Rep}_B \biggl( \begin{bmatrix}
  a \amp b \\ c \amp d
  \end{bmatrix}\biggr) \amp = \begin{bmatrix}
  a \\ b \\ c \\ d
  \end{bmatrix}
  </md>
  where <m>B</m> is the natural basis of <m>\mathcal{M}_{2 \times 2}</m>.  So the matrix trace can be written as

  <md>
  \begin{bmatrix}
  1 \amp 0 \amp 0 \amp 1
  \end{bmatrix}\text{Rep}_B (A) = \begin{bmatrix}
  1 \amp 0 \amp 0 \amp 1
  \end{bmatrix}\begin{bmatrix}
  a \\ b \\ c \\ d
  \end{bmatrix} = a + d
  </md>


  <section><title>Projection Maps</title>

  There is a class of linear transformations that are very important and have a nice geometric interpretation called projection maps.   Let's look at an example in the <m>xy</m>-plane.  Consider the point <m>(1,4)</m> as shown below.

  \begin{center}
  \begin{tikzpicture}
  \draw[->] (-2,0) -- (5,0) node [above right] {<m>x</m>};
  \foreach \x in {-1,1,2,3,4} \draw (\x,0.1) -- (\x,-0.1) node [below] {\x};
  \draw[->] (0,-2) -- (0,5) node [above right] {<m>y</m>};
  \foreach \y in {-1,1,2,3,4} \draw (0.1,\y) -- (-0.1,\y) node [left] {\y};

  \fill (1,4) circle (1.5pt);
  \draw[->,thick] (0,0) -- (1,4) node [midway, right] {<m>\vec{u}</m>};

  \fill (1,0) circle (1.5pt);
  \draw[->,thick] (0,0) -- (1,0);

  \end{tikzpicture}
  \end{center}

  If the point (or the vector) is projected onto the <m>x</m>-axis, then the result is the point <m>(1,0)</m> found by taking the <m>x</m>-component of the point.  In term of the vector, this is <m>[1\;\;0]^T</m>.

  We now look at projecting a vector <m>\vec{u}</m> onto a line (or a vector <m>\vec{v}</m>).  We will derive this in the <m>xy</m>-plane, however the result will work for any vector space.

  \begin{center}
  \begin{tikzpicture}
  \draw[->] (-2,0) -- (5,0) node [above right] {<m>x</m>};

  \draw[->] (0,-2) -- (0,5) node [above right] {<m>y</m>};
  \draw[->,thick] (0,0) -- (1,4) node [midway, right] {<m>\vec{u}</m>};

  \draw[->,thick] (0,0) -- (4,2) node [right] {<m>\vec{v}</m>};


  \draw[->,very thick] (0,0) -- ({52/20},{26/20}) node [below right] {<m>\text{Proj}_{\vec{v}} \vec{u}</m>};

  \draw[dashed] ({52/20},{26/20}) -- (1,4);

  \draw[->,very thick] (0,0) -- ({1-52/20},{4-26/20}) node [above right] {<m>\vec{u}_{\perp}</m>};

  \end{tikzpicture}
  \end{center}

  Let <m>\vec{u}</m> and <m>\vec{v}</m> be two vectors in <m>\mathbb{R}^2</m> as shown above.  We seek the projection of <m>\vec{u}</m> onto <m>\vec{v}</m> and denote this as <m>\text{Proj}_{\vec{v}} \vec{u}</m>.  The projection is a vector that is in the same direction as <m>\vec{v}</m>.  The length will be explained below.

  Any vector <m>\vec{u}</m> can be split into two parts, <m>\vec{u}_{||}</m> a vector in a direction parallel to <m>\vec{v}</m> and a vector <m>\vec{u}_{\perp}</m> which satisfies <m>\langle \vec{u}_{\perp} , \vec{v} \rangle = 0</m>.

  <md> \label{eq:proj:derive}
  \vec{u} = \vec{u}_{||} + \vec{v}_{\perp}
  </md>
  where <m>\vec{u}_{||} = \alpha \vec{v}</m>.

  Take the inner product of both sides of (<xref ref="eq-proj-derive" />) with the vector <m>\vec{v}</m>
  <md>
  \langle \vec{u}, \vec{v} \rangle \amp = \langle \vec{u}_{||} + \vec{u}_{\perp}, \vec{v} \rangle \\
  \amp = \langle \alpha \vec{v}, \vec{v} \rangle + \langle \vec{u}_{\perp}, \vec{v} \rangle \\
  \amp = \alpha \langle \vec{v},\vec{v} \rangle + 0
  </md>
  and solving for <m>\alpha</m>,

  <md>
  \alpha \amp = \frac{ \langle \vec{u}, \vec{v} \rangle}{\langle \vec{v}, \vec{v} \rangle}
  </md>
  Therefore the projection vector

  <md> \label{eq:proj:vector}
  \text{Proj}_{\vec{v}} \vec{u} = \frac{ \langle \vec{u}, \vec{v} \rangle}{\langle \vec{v}, \vec{v} \rangle}  \vec{v}
  </md>
  and if needed the vector <m>\vec{u}_{\perp}</m> is

  <md>
  \vec{u}_{\perp} = \vec{u} - \text{Proj}_{\vec{v}} \vec{u} = \vec{u} - \frac{ \langle \vec{u}, \vec{v} \rangle}{\langle \vec{v}, \vec{v} \rangle}  \vec{v}
  </md>

  Although we derived this projection in <m>\mathbb{R}^2</m>, there is nothing about the projection formula to indicate it's confined to <m>\mathbb{R}^2</m>.  In fact, as we will see, the projection mapping is a linear transformation and applies to vectors in an inner product space.

  <remark>
  The projection of any vector <m>\vec{u} \in \mathbb{R}^n</m> onto a vector <m>\vec{v} \in \mathbb{R}^n</m> is given by

  <md>
  \text{Proj}_{\vec{v}} \vec{u} \amp = \frac{\langle \vec{u},\vec{v} \rangle}{\langle \vec{v}, \vec{v} \rangle} \vec{v}
  </md>
  and the vector perpendicular to <m>\text{Proj}_{\vec{v}}\vec{u}</m> that satisfies <m>\vec{u} = \text{Proj}_{\vec{v}}\vec{u} + \vec{u}_{\perp}</m> is given by

  <md>
  \vec{u}_{\perp} = \vec{u} - \text{Proj}_{\vec{v}} \vec{u} = \vec{u} - \frac{ \langle \vec{u}, \vec{v} \rangle}{\langle \vec{v}, \vec{v} \rangle}  \vec{v}
  </md>

  </remark>



  The example below finds the projection of vectors in <m>\mathbb{R}^3</m>.

  <example>
  Find the projection of <m>\vec{u} = [1\;\;2\;\;3]^T</m> onto the vector <m>\vec{v}=[-2\;\; 0\;\; 4]</m>.

  \solution
  Since
  <md>
  \langle \vec{u}, \vec{v} \rangle \amp = -2+0+12=10 \amp \langle \vec{v},\vec{v} \rangle \amp = (-2)^2 + 0^2 + 4^2 =20.
  </md>
  <md>
  \text{Proj}_{\vec{v}} \vec{u} =  \frac{10}{20} \vec{v} = \frac{1}{2} \begin{bmatrix}
  -2 \\ 0 \\ 4
  \end{bmatrix} = \begin{bmatrix}
  -1 \\ 0 \\ 2
  \end{bmatrix}
  </md>

  </example>



  <theorem> \label{thm:proj:map}
  Given a nonzero vector <m>\vec{v}</m> and a vector <m>\vec{u}</m> both in <m>\mathbb{R}^n</m>, the projection map in (<xref ref="eq-proj-vector" />) is a linear transformation.
  </theorem>

  <proof> We need to prove the two properties of linear transformations in Definition <xref ref="def-linear-trans" />.
  <md>
  \text{Proj}_{\vec{v}}(\vec{u} + \vec{w}) \amp = \frac{\langle (\vec{u}+\vec{w}), \vec{v} \rangle}{\langle \vec{v},\vec{v} \rangle} \\
  \amp = \frac{\langle \vec{u},\vec{v} \rangle + \langle \vec{w},\vec{v} \rangle}{\langle \vec{v},\vec{v} \rangle }  \\
  \amp = \frac{\langle \vec{u},\vec{v} \rangle}{\langle \vec{v},\vec{v} \rangle } + \frac{\langle \vec{w},\vec{v} \rangle}{\langle \vec{v},\vec{v} \rangle } \\
  \amp = \text{Proj}_{\vec{v}} \vec{u} + \text{Proj}_{\vec{v}} \vec{w}
  </md>

  <md>
  \text{Proj}_{\vec{v}} (\alpha \vec{u}) \amp =  \frac{\langle (\alpha \vec{u}), \vec{v} \rangle}{\langle \vec{v},\vec{v} \rangle} \\
  \amp = \alpha \frac{\langle  \vec{u}, \vec{v} \rangle}{\langle \vec{v},\vec{v} \rangle}  = \alpha \text{Proj}_{\vec{v}} \vec{u}
  </md>
  </proof>

  The example shown so far for projections have been onto a vector (which can be thought of as the line through the origin with that direction), but in general, one can project onto any linear space (more technically, any inner product space).  Before showing the general projection, let's consider the projection of a vector onto a plane.  The following example shows this.




  <example>
  Find the projection of vector <m>[6,-2,4]^T</m> onto plane given by <m>x+2y+3z=0</m>.

   \solution

   First, let's find two vectors in the plane, called <m>P</m>, that form an orthogonal basis (we will see why it's helpful to have an orthogonal basis later).  Recall that a normal vector to the plane is given by the vector <m>[1,2,3]^T</m>.  Two vector that are normal to it is <m>\vec{u}_1=[3,0,-1]</m> and <m>\vec{u}_2=[0,-3,2]^T</m>.  (Note: these are two points in the plane).  To form an orthonormal basis, we need to perform Gramm-Schmidt.

  <md>
   \vec{v}_1 \amp = \vec{u}_1
  \amp \vec{v}_2 \amp= \vec{u}_2 - \frac{\langle \vec{v}_1,\vec{u}_2 \rangle}{\langle \vec{v}_1, \vec{v}_1 \rangle} \vec{v}_1  \\
   \amp = \begin{bmatrix}
   3 \\ 0 \\ -1
   \end{bmatrix}\amp
   \vec{v}_2 \amp = \begin{bmatrix}
   0 \\ -3 \\ 2
  \end{bmatrix} - \biggl(-\frac{1}{5}\biggr)
  \begin{bmatrix}
   3 \\ 0 \\ -1
  \end{bmatrix} \\
  \amp\amp\amp  =
  \begin{bmatrix}
   3/5 \\ -3 \\ 9/5
  \end{bmatrix}
  </md>
  and to make the second vector nicer, we'll multiply by <m>5/3</m>.  An orthogonal set in the plane is

  <md>
  \left\{ \begin{bmatrix}
  3 \\ 0 \\ -1
  \end{bmatrix} , \begin{bmatrix}
  1 \\ -5 \\ 3
  \end{bmatrix} \right \}
  </md>
  To consider how to project the vector <m>\vec{u} = [1,2,3]^T</m> into the plane we will write
  <md>
  \vec{u} \amp = \vec{u}_{\perp} + \vec{u}_{||}
  </md>
  where <m>\vec{u}_{\perp}</m> is orthogonal to both <m>\vec{v}_1</m> and <m>\vec{v}_2</m> and we can write

  <md>
  \text{Proj}_{P}\vec{u} = \vec{u}_{||} \amp = c_1 \vec{v}_1 + c_2 \vec{v}_2
  </md>
  So we next take the inner product of <m>\vec{u}</m> with both <m>\vec{v}_1</m> and <m>\vec{v}_2</m>.
  <md>
  \langle \vec{u}, \vec{v}_1 \rangle \amp = \langle \vec{u}_{\perp} + \vec{u}_{||}, \vec{v}_1 \rangle \amp
  \langle \vec{u}, \vec{v}_2 \rangle \amp = \langle \vec{u}_{\perp} + \vec{u}_{||}, \vec{v}_2 \rangle \\
  \amp = \langle \vec{u}_{\perp}, \vec{v}_1 \rangle + \langle \vec{u}_{||}, \vec{v}_1 \rangle \amp
  \amp = \langle \vec{u}_{\perp}, \vec{v}_2 \rangle + \langle \vec{u}_{||}, \vec{v}_2 \rangle \\
  \amp = 0 + \langle c_1 \vec{v}_1 + c_2 \vec{v}_2 ,\vec{v}_1 \rangle \amp
  \amp = 0 + \langle c_1 \vec{v}_1 + c_2 \vec{v}_2 ,\vec{v}_2 \rangle \\
  \amp = c_1 \langle \vec{v}_1, \vec{v}_1 \rangle + c_2 \langle \vec{v}_2, \vec{v}_1 \rangle \amp
  \amp = c_1 \langle \vec{v}_1, \vec{v}_2 \rangle + c_2 \langle \vec{v}_2, \vec{v}_2 \rangle \\
  \amp = c_1 \langle \vec{v}_1, \vec{v}_1 \rangle + 0 \amp
  \amp = 0 + c_2 \langle \vec{v}_2, \vec{v}_1 \rangle
  </md>
  Therefore
  <md>
  c_1 \amp = \frac{\langle \vec{u},\vec{v}_1 \rangle}{\langle \vec{v}_1, \vec{v}_1 \rangle} \amp
  c_2 \amp = \frac{\langle \vec{u},\vec{v}_2 \rangle}{\langle \vec{v}_2, \vec{v}_2 \rangle} \\
  </md>
  So in the case of projecting a vector onto a plane, <m>P</m>,
  <md>
  \text{Proj}_{P} \vec{u} \amp = c_1 \vec{v}_1 + c_2 \vec{v}_2 \\
  \amp = \frac{\langle \vec{u},\vec{v}_1 \rangle}{\langle \vec{v}_1, \vec{v}_1 \rangle} \vec{v}_1 +
  \frac{\langle \vec{u},\vec{v}_2 \rangle}{\langle \vec{v}_2, \vec{v}_2 \rangle} \vec{v}_2 \\
  \amp = \frac{14}{10} \begin{bmatrix}
  3 \\ 0 \\ -1
  \end{bmatrix} + \frac{28}{35} \begin{bmatrix}
  1 \\ -5 \\ 3
  \end{bmatrix} = \begin{bmatrix}
  5 \\-4 \\ 1
  \end{bmatrix}
  </md>
  So the vector <m>[5,-4,1]^{\intercal}</m> is the projection of the vector <m>[1,2,3]^{\intercal}</m> onto the plane <m>x+2y+3z=0</m>.
  </example>






  <subsection><title>Using Projections to solve Least-Squares Problems</title>

  A useful problem is to find the minimum distance between a point and line.  For example consider the point <m>(1,4)</m> and the line <m>L:  y= \frac{1}{2} x</m> as shown in the figure below.

  \begin{center}
  \begin{tikzpicture}
  \draw[->] (-2,0) -- (5,0) node [above right] {<m>x</m>};
  \foreach \x in {-1,1,2,3,4} \draw (\x,0.1) -- (\x,-0.1) node [below] {\x};
  \draw[->] (0,-2) -- (0,5) node [above right] {<m>y</m>};
  \foreach \y in {-1,1,2,3,4} \draw (0.1,\y) -- (-0.1,\y) node [left] {\y};

  \fill (1,4) circle (1.5pt);
  \draw[<->,thick] (-2,-1) -- (4,2) node [right] {<m>L</m>};

  \end{tikzpicture}
  \end{center}

  If we want to minimize the distance from the point <m>P</m> to the line <m>L</m>, we often minimize the square\footnote{Recall that the point that minimizes a function is the same point that to minimizes the square of the function.  The reason for doing this it it gets rid of the square root and makes calculations easier to do.}  of the distance from the point to the line or

  <md>
  g(x) \amp = (x-1)^2 + \biggl(\frac{1}{2} x - 4\biggr)^2
  </md> <!-- g' = 2(x-1) + 2(1/2 x - 4) = 2x-2+x-8 = 3x-11 -->
  and using techniques from calculus, this function is minimized when <m>x=12/5</m>.  Looking at the plot, the <m>y</m>-value is <m>12/5</m> and the vector from <m>(12/5,6/5)</m> to <m>(1,4)</m> is perpendicular to the vector in the direction of <m>L</m> as shown below:

  \begin{center}
  \begin{tikzpicture}
  \draw[->] (-2,0) -- (5,0) node [above right] {<m>x</m>};
  \foreach \x in {-1,1,2,3,4} \draw (\x,0.1) -- (\x,-0.1) node [below] {\x};
  \draw[->] (0,-2) -- (0,5) node [above right] {<m>y</m>};
  \foreach \y in {-1,1,2,3,4} \draw (0.1,\y) -- (-0.1,\y) node [left] {\y};

  \fill (1,4) circle (1.5pt);
  \draw[<->,thick] (-2,-1) -- (4,2) node [right] {<m>L</m>};

  \draw[->,thick] ({12/5},{6/5}) -- (1,4);

  \fill[red] (2.4,1.2) circle (2pt);
  \end{tikzpicture}
  \end{center}

  In light of projections, we can reframe this problem.  We are seeking the point on <m>L</m> closest to <m>P</m>.  This can be found by projecting the vector <m>\overrightarrow{OP}</m> to the line <m>L</m> or

  <md>
  \text{proj}_{\vec{v}} \overrightarrow{OP} = \frac{\langle \vec{v}, \overrightarrow{OP} \rangle} {\langle \vec{v},\vec{v}\rangle} \vec{v}
  </md>
  and in this case, with <m>\vec{v} = [2\;\;1]^{\intercal}</m> and <m>\overrightarrow{OP}=[1\;\;4]^{\intercal}</m>,
  <md>
  \text{proj}_{\vec{v}} \overrightarrow{OP} = \frac{\langle \vec{v}, \overrightarrow{OP} \rangle} {\langle \vec{v},\vec{v}\rangle} = \frac{6}{5} \begin{bmatrix}
  2 \\ 1
  \end{bmatrix}= \begin{bmatrix}
  12/5 \\ 6/5
  \end{bmatrix}
  </md>
  which is the same result as from Calculus.


  <subsection><title>Projecting onto a Vector Space</title>

  We again generalize from projecting onto a single vector or set of vectors to a general vector space, <m>V</m>.  First, to make things easier, we will use an orthogonal basis for <m>V</m>, call it, <m>( \vec{v}_1, \vec{v}_2, \ldots, \vec{v}_n )</m>.

  Recall that a projection onto a vector is found by writing the original vector, <m>\vec{u}=\vec{u}_{||} + \vec{u}_{\perp}</m>, where <m>\vec{u}_{||}</m> is in <m>V</m> or

  <md>
  \vec{u} \amp = \vec{u}_{\perp} + \sum_{i=1}^n c_i \vec{v}_i
  \label{eq:projection:vector:space}
  </md>
  Finding the projection is analogous to finding the constants <m>c_i</m>.  Take the inner product of (<xref ref="eq-projection-vector-space" />) with <m>v_k</m>, the <m>k</m>th basis vector of <m>V</m> or

  <md>
  \langle \vec{u}, \vec{v}_k \rangle \amp= \biggl\langle \vec{u}_{\perp} + \sum_{i=1}^n c_i \vec{v}_i , \vec{v}_k \biggr\rangle \intertext{using properties of inner products}
  \amp = \langle \vec{u}_{\perp}, \vec{v}_k \rangle + \sum_{i=1}^n c_i \langle \vec{v}_i, \vec{v}_k \rangle
  \intertext{since <m>\vec{u}_{\perp}</m> is chosen to be orthogonal to the vector space and since the basis of <m>V</m> is orthogonal,}
  \amp = c_k \langle v_k, v_k \rangle
  </md>
  Therefore
  <md>
  c_k \amp = \frac{\langle \vec{u}, \vec{v}_k \rangle}{\langle \vec{v}_k, \vec{v}_k \rangle}
  </md>
  and thus we can make the following statement about projecting any element onto a subspace:

  <remark>
  Let <m>\vec{u}</m> be an element in a vector space <m>S</m> and <m>( \vec{v}_1, \vec{v}_2, \ldots, \vec{v}_n )</m> be an orthogonal basis of a subspace <m>V</m>.  The projection of <m>\vec{u}</m> onto <m>V</m> can be written:
  <md>
  \text{Proj}_V (\vec{u}) \amp = \sum_{i=1}^n c_k v_k
  </md>
  where
  <md>
  c_k \amp = \frac{\langle \vec{u}, \vec{v}_k \rangle}{\langle \vec{v}_k, \vec{v}_k \rangle}
  </md>

  Note: if the basis of <m>V</m> is also orthonormal, then <m>c_k = \langle \vec{u}, \vec{v}_k \rangle</m>.
  </remark>


  <example>
  Find the projection of <m>\sin \pi x</m>, an element of <m>{\cal C}^{(0)}</m> onto <m>{\cal P}_5[-1,1]</m>

  \solution

  First, we need a orthogonal basis of the subspace.  This was found above and we can use the Legendre polynomials or
  <md>
  (1,x,3x^2-1,5x^3-3x,35x^4-30x^2+3,63x^5-70x^3+15x) \label{eq:legendre:poly}
  </md>

  Next, find all of the values of <m>c_k</m> for <m>k=0,1,2,3,4,5</m>
  <md>
  c_0 \amp = \frac{\langle \sin \pi x, 1 \rangle }{\langle 1, 1 \rangle} = \frac{\int_{-1}^1 \sin \pi x \, dx}{\int_{-1}^1 1^2 \, dx} = 0 \\
  c_1 \amp = \frac{\langle \sin \pi x, x \rangle }{\langle x, x \rangle} = \frac{\int_{-1}^1 x \sin \pi x \, dx}{\int_{-1}^1 x^2 \, dx} = \frac{2/\pi}{2/3} =\frac{3}{\pi}
  </md>
  Similarly, it can be shown that
  <md>
  c_2 \amp = c_4 = 0 \\
  c_3 \amp = \frac{7(\pi^2-15)}{\pi^3} \amp c_5 \amp = \frac{11(\pi^4-105\pi^2+945)}{\pi^5}
  </md>

  The projection then is the sum

  <md>
  \sum_{i=0}^5 c_k P_k(x)
  </md>
  where <m>P_k</m> is the <m>k</m>th Legendre polynomial as in (<xref ref="eq-legendre-poly" />).

  A plot of this is

  \begin{center}
  \begin{tikzpicture}[xscale=4,yscale=2]
  \draw[->] (-1.1,0) -- (1.1,0) node [above right] {<m>x</m>};
  \foreach \x in {-1,-0.5,...,1} \draw (\x,0.1) -- (\x,-0.1) node [below] {\x};
  \draw[->] (0,-1.5) -- (0,1.5) node [above right] {<m>y</m>};
  \foreach \y in {-1,-0.5,...,1} \draw (0.1,\y) -- (-0.1,\y) node [left] {\y};
  \draw plot[domain=-1:1,smooth] (\x,{\x*(3.103460404+(-4.814388214+1.726905092*pow(\x,2))*pow(\x,2))});
  \end{tikzpicture}
  \end{center}
  which looks quite sine-like and is fairly close if we plot
  <md>
  |\sin \pi x -\sum_{i=0}^5 c_k P_k(x)|
  </md>

  \begin{center}
  \begin{tikzpicture}[xscale=4,yscale=300]
  \draw[->] (-1.1,0) -- (1.1,0) node [above right] {<m>x</m>};
  \foreach \x in {-1,-0.5,...,1} \draw (\x,0.0005) -- (\x,-0.0005) node [below] {\x};
  \draw[->] (0,-0.001) -- (0,0.012) node [above right] {<m>y</m>};
  \foreach \y in {0.0025,0.005,0.0075,0.01} \draw (0.025,\y) -- (-0.025,\y) node [left] {\y};
  \draw plot[domain=-1:1,samples=400] (\x,{abs(sin(pi*\x r)-\x*(3.103460404+(-4.814388214+1.726905092*pow(\x,2))*pow(\x,2)))});
  \end{tikzpicture}
  \end{center}


  </example>

  This last example shows the power of projections.  If we are using functions as elements of vector spaces, then a projection of a function onto a vector space (using the span of a vector space), is the closest function in the vector space to the original function.  In Chapter <xref ref="ch-fourier-series" />, we will do this with trigonometric function and is called Fourier Series.

  <!--
  %In general this applies to any inner product space.  We can take any point (thought of as a point vector) in the space and project it onto a subspace.  The resulting vector (thought of also as a point) is the point in the subspace closest to the original point. The term closest requires a distance, which is the norm as explained in the following theorem.
  %
  %<theorem>
  %Consider <m>S</m> a subspace of <m>\mathbb{R}^n</m>.  If <m>\vec{s} \in S</m>, then, for any <m>\vec{x} \in \mathbb{R}^2</m> then the <m>\vec{s}</m> that satisfies
  %%
  %<md>
  %\min_{\vec{s} \in S} ||\vec{x}-\vec{s}||
  %</md>
  %is given by
  %%
  %<md>
  %\vec{s} = \text{proj}_S \vec{x}
  %</md>
  %</theorem>
  %
  %<proof>
  %Let <m>E</m> be the square of the norm that represents the distance between the point <m>\vec{x}</m> and any <m>\vec{s} \in S</m>.
  %
  %<md>
  %E(\vec{s}) \amp = || \vec{s} - \vec{x}|| = \langle \vec{s}-\vec{x},\vec{s} - \vec{x} \rangle \\
  %\amp = \langle \vec{s},\vec{s} \rangle - 2 \langle \vec{x},\vec{s}\rangle + \langle \vec{x}, \vec{x} \rangle
  %</md>
  %If we differentiate <m>E</m> with respect to <m>\vec{s}</m>, then the result is:
  %%
  %<md>
  %\frac{\partial E}{\partial \vec{s}} \amp  = 2 \vec{s} -
  %</md>
  %
  %{\color{red} ARRGGGG!!  Maybe write as a projection matrix first.  }
  %</proof>
  %


  %<subsection><title>Projection Matrices</title>
  %
  %Since from Theorem <xref ref="thm-proj-map" />, that the projection map is a linear transformation and from Theorem \ref{thm:linear:trans:matrix} that a linear transformation, we examine the projection map as a matrix.  Let's first consider the projection of
  %
  %{\color{red} Not sure what I want to do here.}
  %
  %<example>
  % The transformation matrices for the previous theorems are:
  %
  %<ol>
  %
  %\item
  % %
  %<md>
  %\begin{bmatrix}
  % 1 \amp 0 \\ 0 \amp -1
  %\end{bmatrix}
  %</md>
  %
  %\item
  %<md>
  %\begin{bmatrix}
  % \cos \theta \amp -\sin \theta \\ \sin \theta \amp \cos \theta
  %\end{bmatrix}
  %</md>
  %
  %\item To show that the projection operation above can be written as a matrix, let <m>\vec{v} = [v_1,v_2,v_3]^T</m> and write:
  %%
  %<md>
  % P \vec{v} \amp = A_T \vec{v}  =
  %\begin{bmatrix}
  % a_{11} \amp a_{12} \amp a_{13} \\
  % a_{21} \amp a_{22} \amp a_{23} \\
  % a_{31} \amp a_{32} \amp a_{33} \\
  %\end{bmatrix}
  %\begin{bmatrix}
  % v_1 \\ v_2 \\v_3
  %\end{bmatrix}  =
  %\begin{bmatrix}
  % a_{11} v_1 + a_{12} v_2 + a_{13} v_3 \\
  % a_{21} v_1 + a_{22} v_2 + a_{23} v_3 \\
  % a_{31} v_1 + a_{32} v_2 + a_{33} v_3 \\
  %\end{bmatrix}
  %\\
  %\amp  =
  %\begin{bmatrix}
  % v_1 \\ v_2 \\v_3
  %\end{bmatrix}^T
  %\frac{1}{\sqrt{10}}
  %\begin{bmatrix}
  % 3 \\ 0 \\ -1
  %\end{bmatrix}\frac{1}{\sqrt{10}}
  %\begin{bmatrix}
  % 3 \\ 0 \\ -1
  %\end{bmatrix} +
  %\begin{bmatrix}
  % v_1 \\ v_2 \\v_3
  %\end{bmatrix}^T \frac{1}{\sqrt{35}}
  %\begin{bmatrix}
  % 1 \\ -5 \\ 3
  %\end{bmatrix} \frac{1}{\sqrt{35}}
  %\begin{bmatrix}
  % 1 \\ -5 \\ 3
  %\end{bmatrix} \\
  %\amp = \frac{1}{10} (3 v_1 -v_3)
  %\begin{bmatrix}
  % 3 \\ 0 \\ -1
  %\end{bmatrix} + \frac{1}{35} (v_1 - 5v_2 + 3v_3)
  %\begin{bmatrix}
  % 1 \\ -5 \\ 3
  %\end{bmatrix} \\
  %\amp =
  %\begin{bmatrix}
  % (9/10 + 1/35)  v_1 - 1/7 v_2 + (3/35-3/10) v_3 \\
  % -1/7 v_1 + 5/7 v_2 -3/7 v_3 \\
  % (3/10+3/35) v_1 -3/7 v_2 + (1/10 +9/35)v_3
  %\end{bmatrix}  \\ \amp =
  %\begin{bmatrix}
  %13/14 \amp -1/7 \amp -3/14 \\
  % -1/7 \amp 5/7 \amp -3/7 \\
  %-3/14 \amp -3/7 \amp 5/14
  %\end{bmatrix}
  %\begin{bmatrix}
  % v_1 \\ v_2 \\ v_3
  %\end{bmatrix}
  %</md>
  %</ol>
  %</example> -->