<?xml version="1.0" encoding="UTF-8" ?>
<section>
	<title>Singular Value Decomposition</title>
	<introduction>
		<p>
			The singular value decomposition, also known as the SVD, is yet another way to factor a matrix.
			In short, a matrix <m>A</m> can be written <m>A=U \Sigma V^{\intercal}</m>, where <m>U</m> and <m>V</m>
			are orthogonal matrices and <m>\Sigma</m> is a diagonal matrix.
			These can be thought of as a rotation, followed by a scaling, followed by another rotation.
			It has numerous applications as we will see at the end of this section.
		</p>

		<p>
			We will show that it is possible to factor any <m>m</m> by <m>n</m> matrix <m>A</m> in this
			form and also determine how to find the matrices <m>U</m>, <m>\Sigma</m> and <m>V</m>.
			Consider a real <m>m</m> by <m>n</m> matrix <m>A</m>.
			We will consider <m>U</m>, an <m>m</m> by <m>m</m> matrix, <m>\Sigma</m>, an <m>m</m> by
			<m>n</m> matrix and <m>V</m> a <m>n</m> by <m>n</m> matrix.
		</p>

		<theorem xml:id="thm-svd">
			<title>Singular Value Decomposition</title>
			<statement>
				<p>
					Let <m>A</m> be a general <m>m</m> by <m>n</m> matrix.
					The matrix can be written:
				</p>

				<p>
					<men xml:id="eq-SVD-decomp">
						A = U \Sigma V^{\intercal}
					</men>
				</p>

				<p>
					where <m>U</m> and <m>V</m> are square orthogonal matrices of size <m>m</m>
					and <m>n</m> respectively and <m>\Sigma</m> is an <m>m</m> by <m>n</m> diagonal matrix.
					The columns of <m>U</m> are the eigenvectors of <m>AA^{\intercal}</m> and the columns
					of <m>V</m> are the eigenvectors of <m>A^{\intercal}A</m>.
				</p>
			</statement>
		</theorem>

		<p>
			The general proof is not presented here.
			Instead, assume that the matrix can be
			factored as in the Singular Value Decomposition, then
		</p>

		<p>
			<md>
				<mrow>AA^{\intercal} \amp = U \Sigma V^{\intercal} (U \Sigma V^{\intercal})^{\intercal}</mrow>
				<mrow>\amp = U \Sigma V^{\intercal} V \Sigma^{\intercal} U^{\intercal}</mrow>
				<mrow>\text{and since $V^{\intercal}V=I$, then }</mrow>
				<mrow number="yes" xml:id="eq-SVD-similar">AA^{\intercal}\amp = U \Sigma \Sigma^{\intercal} U^{\intercal}.</mrow>
			</md>
		</p>

		<p>
			The astute reader will notice that <xref ref="eq-SVD-similar" /> shows that the matrices
			<m>AA^{\intercal}</m> and <m>\Sigma \Sigma^{\intercal}</m> are similar and thus have the
			same eigenvalues.
			Recall also that <m>U</m> will be the eigenvectors of <m>AA^{\intercal}</m>.
			These properties are used to actually find <m>U</m> and <m>\Sigma</m>.
			Also, if you can find either <m>U</m> or <m>V</m>, then finding the other is easier by
			solving <xref ref="eq-SVD-decomp" />.
			If we know <m>V</m>, then
		</p>

		<p>
			<md>
				<mrow>U \Sigma V^{\intercal} \amp = A</mrow>
				<mrow>\Sigma V^{\intercal}  \amp = U^{-1} A </mrow>
			</md>
		</p>

		<p>
			and recall that since <m>U</m> is an orthogonal matrix that <m>U^{-1}=U^{\intercal}</m>.
			This is now left-multiplied by <m>\Sigma^{\intercal}</m> in case the original matrix is not square,
			then <m>\Sigma</m> is not square and the inverse is not defined.
		</p>

		<p>
			<md>
				<mrow>\Sigma^{\intercal} \Sigma V^{\intercal}  \amp = \Sigma^{\intercal} U^{\intercal} A</mrow>
				<mrow>V^{\intercal} \amp = (\Sigma^{\intercal} \Sigma)^{-1} \Sigma^{\intercal} U^{\intercal} A </mrow>
				<mrow>V \amp = ((\Sigma^{\intercal} \Sigma)^{-1} \Sigma^{\intercal} U^{\intercal} A)^{\intercal}</mrow>
				<mrow number="yes" xml:id="eq-SVD-V">V \amp = A^{\intercal} U \Sigma ((\Sigma^{\intercal} \Sigma)^{-1})^{\intercal}</mrow>
			</md>
		</p>

		<p>
			where the fourth property of <xref ref="lem-matrix-transpose" /> was used.
			Similarly, if one solves for <m>U</m>:
		</p>

		<p>
			<md>
				<mrow>U \Sigma V^{\intercal} \amp = A </mrow>
				<mrow>U \Sigma V^{\intercal}V \amp = AV </mrow>
				<mrow>U \Sigma \amp = A V </mrow>
				<mrow>U \Sigma \Sigma^{\intercal} \amp = A V \Sigma^{\intercal} </mrow>
				<mrow number="yes" xml:id="eq-SVD-U">U \amp = A V \Sigma^{\intercal} (\Sigma \Sigma^{\intercal})^{-1} </mrow>
			</md>
		</p>

		<p>
			where <m>V^{-1}=V^{\intercal}</m> is used on the 3rd step since <m>V</m> is an orthogonal matrix.
		</p>

		<p>
			Two examples of finding the SVD are presented below.
			The first is for a square matrix and the second is for a non-square matrix.
			We first start with the SVD of a 2 by 2 matrix with the following example.
		</p>

		<example xml:id="ex-svd-2by2">
			<statement>
				<p>
					Find <m>U, \Sigma,</m> and <m>V</m> of the SVD for the matrix
				</p>

				<p>
					<me>
						A  = \begin{bmatrix}
						-1 \amp 2 \\
						1 \amp 2
						\end{bmatrix}
					</me>
				</p>
			</statement>

			<solution>
				<p>
					As explained above, the columns of <m>U</m> are the eigenvectors of <m>AA^{\intercal}</m> and
				</p>

				<p>
					<me>
						A A^{\intercal} = \begin{bmatrix}
						5 \amp 3 \\
						3 \amp 5
						\end{bmatrix}
					</me>
				</p>

				<p>
					The eigenvalues are found by
				</p>

				<p>
					<me>
						\det(A-\lambda I) = \begin{vmatrix}
						5 - \lambda \amp 3 \\ 3 \amp 5- \lambda
						\end{vmatrix} = (5-\lambda)^2 - 9  = 0
					</me>
				</p>

				<p>
					with solutions <m>\lambda = 2, 8</m>.
					If <m>\lambda = 2</m>, then the solution to
				</p>

				<p>
					<me>
						(AA^{\intercal}-2I)\boldsymbol{v}_1  = \begin{bmatrix}
						3 \amp 3 \\ 3 \amp 3
						\end{bmatrix}\boldsymbol{v}_1 = \begin{bmatrix}
						0 \\ 0
						\end{bmatrix}
					</me>
				</p>

				<p>
					is <m>\boldsymbol{v}_1 = [1,-1]^{\intercal}</m>.
					And if <m>\lambda = 8</m>, then the solution to
				</p>

				<p>
					<me>
						(AA^{\intercal}-8I) \boldsymbol{v}_2  = \begin{bmatrix}
						-3 \amp 3 \\
						3 \amp -3
						\end{bmatrix} \boldsymbol{v}_2  = \begin{bmatrix}
						0 \\ 0
						\end{bmatrix}
					</me>
				</p>

				<p>
					is <m>\boldsymbol{v}_2 = [1,1]^{\intercal}</m>.
					Thus the columns of <m>U</m> are the two eigenvectors scaled to be unit vectors,
				</p>

				<p>
					<me>
						U  = \begin{bmatrix}
						1/\sqrt{2} \amp 1/\sqrt{2} \\
						-1/\sqrt{2} \amp 1/\sqrt{2}
						\end{bmatrix}
					</me>
				</p>

				<p>
					The singular values in the matrix <m>\Sigma</m> are the square roots of
					the eigenvalues of <m>AA^{\intercal}</m> and the matrix is
				</p>

				<p>
					<me>
						\Sigma = \begin{bmatrix}
						\sqrt{2} \amp 0 \\
						0 \amp \sqrt{8}
						\end{bmatrix}
					</me>
				</p>

				<p>
					To find the matrix <m>V</m>, we will use (<xref ref="eq-SVD-V" />).
					Since
				</p>

				<p>
					<me>
						\Sigma^{\intercal} \Sigma = \begin{bmatrix}
						2 \amp 0 \\
						0 \amp 8
						\end{bmatrix}
					</me>
				</p>

				<p>
					and
				</p>

				<p>
					<me>
						(\Sigma^{\intercal} \Sigma)^{-1}  = \begin{bmatrix}
						1/2 \amp 0 \\
						0 \amp 1/8
						\end{bmatrix},
					</me>
				</p>

				<p>
					then using <xref ref="eq-SVD-V" />,
				</p>

				<p>
					<md>
						<mrow>V \amp = A^{\intercal} U \Sigma ((\Sigma^{\intercal} \Sigma)^{-1})^{\intercal} \\
						\amp = \begin{bmatrix}
						-1 \amp 1 \\
						2 \amp 2
						\end{bmatrix}
						\begin{bmatrix}
						1/\sqrt{2} \amp 1/\sqrt{2} \\
						-1/\sqrt{2} \amp 1/\sqrt{2}
						\end{bmatrix}
						\begin{bmatrix}
						\sqrt{2} \amp 0 \\
						0 \amp \sqrt{8}
						\end{bmatrix}
						\begin{bmatrix}
						1/2 \amp 0 \\
						0 \amp 1/8
						\end{bmatrix} \\
						\amp = \begin{bmatrix}
						-1 \amp 0 \\ 0 \amp 1
						\end{bmatrix}</mrow>
					</md>
				</p>

				<p>
					It can be checked that indeed <m>A=U\Sigma V^{\intercal}</m>.
				</p>
			</solution>
		</example>

		<p>
			The next example finds the SVD of a 2 by 3 matrix, showing how to handle a non-square matrix.
		</p>

		<example>
			<statement>
				<p>
					Find the SVD of the matrix
				</p>

				<p>
					<me>
						A  = \begin{bmatrix}
						2 \amp -3 \amp -3 \\
						2 \amp 3 \amp 3
						\end{bmatrix}
					</me>
				</p>
			</statement>

			<solution>
				<p>
					In this example, we will first find the columns of <m>V</m> by the eigenvectors of <m>A^{\intercal}A</m>.
					Generally, if the original matrix is not square, one should select the larger matrix
					between <m>AA^{\intercal}</m> and <m>A^{\intercal}A</m>.
				</p>

				<p>
					<me>
						A^{\intercal}A = \begin{bmatrix}
						8 \amp 0 \amp 0 \\
						0 \amp 18 \amp 18 \\
						0 \amp 18 \amp 18
						\end{bmatrix}
					</me>
				</p>

				<p>
					and the eigenvalues of <m>A^{\intercal}A</m> is found by
				</p>

				<p>
					<md>
						<mrow>
						\det(A^{\intercal}A-\lambda I) \amp = \begin{vmatrix}
						8-\lambda \amp 0 \amp 0 \\
						0 \amp 18-\lambda \amp 18 \\
						0 \amp 18 \amp 18-\lambda
						\end{vmatrix} = (8-\lambda)\bigl((18-\lambda)^2-18^2\bigr)</mrow>
						<mrow>\amp = (8-\lambda)(\lambda^2-36\lambda) = 0 </mrow>
					</md>
				</p>

				<p>
					which has the solutions <m>\lambda=8,36,0</m>.
					Thus the singular values result in
				</p>

				<p>
					<me>
						\Sigma = \begin{bmatrix}
						\sqrt{8} \amp 0 \amp 0 \\
						0 \amp 6 \amp 0
						\end{bmatrix}
					</me>
				</p>

				<p>
					and recall that this matrix is always the same size as the original matrix, <m>A</m>.
					Also, you will get zero eigenvalues whenever the matrix is not square.
					The corresponding eigenvectors are
				</p>

				<p>
					<md>
						<mrow>\boldsymbol{v}_1 \amp = \begin{bmatrix}
						1 \\ 0 \\ 0
						\end{bmatrix}, \amp \boldsymbol{v}_2 \amp = \begin{bmatrix}
						0 \\ 1 \\1
						\end{bmatrix} \amp \boldsymbol{v}_3 \amp = \begin{bmatrix}
						0 \\ -1 \\ 1
						\end{bmatrix}</mrow>
					</md>
				</p>

				<p>
					And the matrix <m>V</m> with the columns above scaled to make them unit vectors is
				</p>

				<p>
					<me>
						V  = \begin{bmatrix}
						1 \amp 0 \amp 0 \\
						0 \amp 1/\sqrt{2} \amp -1/\sqrt{2} \\
						0 \amp 1/\sqrt{2} \amp 1/\sqrt{2}
						\end{bmatrix}
					</me>
				</p>

				<p>
					To find <m>U</m>, we will use (<xref ref="eq-SVD-U" />) and note that
				</p>

				<p>
					<me>
						(\Sigma \Sigma^{\intercal})^{-1}  = \begin{bmatrix}
						1/8 \amp 0 \\
						0 \amp 1/36
						\end{bmatrix}
					</me>
				</p>

				<p>
					and now using (<xref ref="eq-SVD-U" />)
				</p>

				<p>
					<md>
						<mrow>U \amp = A V \Sigma^{\intercal} (\Sigma \Sigma^{\intercal})^{-1}  \\
						\amp = \begin{bmatrix}
						2 \amp -3 \amp -3 \\
						2 \amp 3 \amp 3
						\end{bmatrix} \begin{bmatrix}
						1 \amp 0 \amp 0 \\
						0 \amp 1/\sqrt{2} \amp -1/\sqrt{2} \\
						0 \amp 1/\sqrt{2} \amp 1/\sqrt{2}
						\end{bmatrix}\begin{bmatrix}
						\sqrt{8} \amp 0  \\
						0 \amp 6 \\
						0 \amp 0
						\end{bmatrix}\begin{bmatrix}
						1/8 \amp 0 \\
						0 \amp 1/36
						\end{bmatrix} \\
						\amp = \begin{bmatrix}
						\sqrt{2}/2 \amp \sqrt{2}/2 \\
						\sqrt{2}/2 \amp -\sqrt{2}/2
						\end{bmatrix}</mrow>
					</md>
				</p>

				<p>
					and it can be checked that <m>U\Sigma V^{\intercal} = A</m>.
				</p>
			</solution>
		</example>

		<p>
			It is noted that the above two examples result in fairly nice matrices <m>U,V</m> and <m>\Sigma</m>.
			This very rarely happens.
			In addition, as the size of the matrices grow, the eigenvalues generally cannot be found exactly.
			Therefore for matrices of sizes larger than 3 by 3, numerical techniques are used to find the factors.
		</p>
	</introduction>

	<subsection>
		<title>The geometry of the SVD</title>
		<p>
			This section will explain in geometric terms what the SVD generates for an example that can be plotted in 2D.
			First, let's consider 100 points that are scattered around the origin and perhaps they look like the following:
		</p>

		<figure xml:id="fig-random-points">
			<caption>Some Random Points</caption>
			<image width="75%" xml:id="plot-random-points">
				<latex-image><![CDATA[
				\begin{tikzpicture}[scale=0.5]
				\draw[->] (-10.5,0) -- (10.5,0) node [above right] {$x$};
				\foreach \x in {-10,-8,...,-2,2,4,...,10} \draw (\x,0.1) -- (\x,-0.1) node [below] {\x};
				\draw[->] (0,-6.5) -- (0,6.5) node [above right] {$y$};
				\foreach \y in {-6,-4,-2,2,4,6} \draw (0.1,\y) -- (-0.1,\y) node [left] {\y};
				%\begin{luacode}
				%local x
				%local y
				%for i=1,100 do
				%x = 20*math.random()-10
				%y = 0.3*x + (2*math.random()-1)
				%tex.print('\\fill (',x,',',y,') circle[radius=2pt];')
				%end
				%\end{luacode}
				\draw[very thick,->] (0,0) -- (-9.53,-3.01);
				\draw[very thick,->] (0,0) -- (-0.30,0.95);
				\end{tikzpicture}
				]]>
				</latex-image>
			</image>
		</figure>

		<p>
			the matrix is a 2 by 100 matrix with each column the <m>x</m> and <m>y</m> value of each point, we can perform an SVD of this.
			The results for <m>U</m> and the 2 diagonal elements of <m>\Sigma</m> (denoted <m>\sigma_1</m> and <m>\sigma_2</m>) are
		</p>

		<p>
			<md>
				<mrow>\sigma_1 \amp = 54.51177 \amp \sigma_2 \amp =
				5.576430 \amp U \amp = \begin{bmatrix}
				-0.953478 \amp -0.30146\\
				-0.30146 \amp 0.953478
				\end{bmatrix}</mrow>
			</md>
		</p>

		<p>
			where these have been done using software (not by hand).
			If we plot the two vectors in the columns of <m>U</m> with relative sizes of those found in <m>\Sigma</m>,
			you see in the plot above that the larger vector points in the direction of stretch of the
			points and the second vector is orthogonal.
			The relative lengths are the relative stretches of the points.
			The geometry of the columns in the <m>V</m> matrix is more difficult to understand in this situation
			because each of the vectors are length 100.
			If the original matrix had 3 rows, then each column is a point in 3 dimensions.
			The 3 columns in <m>U</m> would again show the stretching of the points in 3 orthogonal
			directions with the stretching factor in the <m>\Sigma</m> matrix.
		</p>
	</subsection>

	<subsection>
		<title>Finding a best-fit line and plane using SVD</title>
		<p>
			You might notice from above that perhaps we can use the SVD to find a line to a set of points and this is true.
			We will use a simpler example to show how this work and how it is similar to other fitting techniques.
			We can also use a similar technique to find the best-fit plane through points in 3 dimensions.
			We first start with a simple example of points in the plan.
			Consider the points (1,2), (2,2), (3,5), (6,7).
			We wish to fit a line that best fits the points.
		</p>

		<figure xml:id="fig-best-fit-points-svd">
			<caption>Best fit points</caption>
			<image width="75%" xml:id="plot-best-fit-points-svd">
				<latex-image>
				<![CDATA[
				\begin{tikzpicture}[scale=1.2]
				\draw[->] (-2,0) -- (7,0) node [above right] {$x$};
				\draw[->] (0,-2) -- (0,7) node [above right] {$y$};
				\foreach \x in {-1,1,2,...,6} {\draw (\x,0.15) -- (\x,-0.15) node [below] {$\x$};}
				\foreach \y in {-1,1,2,...,6} {\draw (0.15,\y) -- (-0.15,\y) node [left] {$\y$};}
				\foreach \x/\y in {1/2,2/2,3/5,6/7} \fill (\x,\y) circle [radius=2.5pt];
				\end{tikzpicture}
				]]>
				</latex-image>
			</image>
		</figure>

		<p>
			First, we will find the center of these points by just finding the average of the <m>x</m> and <m>y</m> values.
		</p>

		<p>
			<md>
				<mrow>\overline{x} \amp = \frac{1+2+3+6}{4} = 3 \amp \overline{y} \amp = \frac{2+2+5+7}{4} = 4</mrow>
			</md>
		</p>

		<p>
			We now generate a new data set where each <m>x</m> and <m>y</m> value is adjusted by the means above.
			The new data set is <m>\{(-2,-2),(-1,-2),(0,1),(3,3)\}</m>.
			We then use these values to find
		</p>

		<p>
			<me>
				A = \begin{bmatrix}
				-2 \amp -1 \amp 0 \amp 3 \\
				-2 \amp -2 \amp 1 \amp 3
				\end{bmatrix}
			</me>
		</p>

		<p>
			As above, the singular values in <m>\Sigma</m> and the matrix <m>U</m> will be most important.
			As above, the matrix <m>U</m> have the eigenvectors of <m>AA^{\intercal}</m> with the
			eigenvalues as the diagonal elements of <m>\Sigma</m>.
		</p>

		<p>
			<me>
				AA^{\intercal} = \begin{bmatrix}
				14 \amp 15 \\ 15 \amp 18
				\end{bmatrix}
			</me>
		</p>

		<p>
			Again, using software, the diagonal elements of <m>\Sigma</m>, labelled <m>\sigma_1</m>
			and <m>\sigma_2</m> and the matrix <m>U</m> is
		</p>

		<p>
			<md>
				<mrow>\sigma_1 \amp = 5.5796 \amp \sigma_2 \amp = 0.93126 \amp U \amp = \begin{bmatrix}
				-0.658724 \amp -0.752384 \\
				-0.752384 \amp 0.658725
				\end{bmatrix}</mrow>
			</md>
		</p>

		<p>
			The prominent eigenvector is the one corresponding to the larger singular value (the first one),
			so the eigenvalue to use is the first column of <m>U</m>.
			The slope of the line will be the <m>y</m>-component over the <m>x</m>-component (rise over run) or
		</p>

		<p>
			<me>
				m = \frac{-0.752384}{-0.658724} = 1.1421830
			</me>
		</p>

		<p>
			Lastly, to find the line, we will use the point-slope form with the average values (or center point) of <m>(3,4)</m>:
		</p>

		<p>
			<md>
				<mrow>y-4 \amp = 1.1428130 (x-3)</mrow>
				<mrow>y \amp = 1.1428130 x + 0.571561</mrow>
			</md>
		</p>

		<p>
			and the following is a plot of the data and the line:
		</p>

		<figure xml:id="fig-points-and-line">
			<caption>Best fit line using SVD</caption>
			<image width="75%" xml:id="plot-points-and-line">
				<latex-image>
				<![CDATA[
				\begin{tikzpicture}[scale=1.2]
				\draw[->] (-2,0) -- (7,0) node [above right] {$x$};
				\draw[->] (0,-2) -- (0,7) node [above right] {$y$};
				\foreach \x in {-1,1,2,...,6} {\draw (\x,0.15) -- (\x,-0.15) node [below] {$\x$};}
				\foreach \y in {-1,1,2,...,6} {\draw (0.15,\y) -- (-0.15,\y) node [left] {$\y$};}
				\foreach \x/\y in {1/2,2/2,3/5,6/7} \fill (\x,\y) circle [radius=2.5pt];
				\draw[<->] plot [domain=-0.5:6] (\x,{1.1428130*\x + 0.571561});
				\end{tikzpicture}
				]]>
				</latex-image>
			</image>
		</figure>

		<p>
			And as a comparison, if we use least-squares to find the line, the result is
		</p>

		<p>
			<me>
				m = 1.0714x+ 0.285714
			</me>
		</p>

		<p>
			The reason for the difference is because each method minimizes something different.
			The least-squares line is the one that minimizes the vertical distance between the data and the line.
			The SVD best-fit line produces a line that minimizes the square of the orthogonal distance to all the points.
		</p>
	</subsection>
</section>